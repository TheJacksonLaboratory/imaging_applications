[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "JAX Research IT, Imaging Applications team",
    "section": "",
    "text": "We are a team of four technical experts inside the Research IT department at The Jackson Laboratory providing support to researchers on all their imaging data needs. Some of our offerings include:\n\nTraining\nWe run about a dozen workshops per year on a variety of imaging-related topics, covering both specific tools and general skills, from beginner level to advanced topics. In 2024, these included, for example:\n\npopular tools such as Fiji, QuPath, CellProfiler, and napari;\nimaging data management with OMERO;\nimage processing with Python, from basic skills to advanced machine learning methods;\nhow to follow coding best practices in Python.\n\n\n\nConsultation\nOur team is routinely in touch with Principal Investigators and their teams, assisting on image analysis projects in steps such as tool selection, pipelining, applying existing pipelines in our computational environments and so on.\n\n\nData management applications\nWe run two instances of OMERO, an open-source microscopy data management platform; one internal to JAX, for day-to-day research usage, and one cloud-based, as a public data repository. JAX researchers can move data between the two instances to publish a dataset with minimal hassle.\n\n\nExpert advice\nOur team takes pride in being current with the state-of-the-art on anything bioimaging, multiplying that knowledge back into the JAX community. Our researchers are rarely bioimage analysis experts, and they don’t need to be; that’s our job!\n\n\nTool development\nWe are deeply involved in the wider open-source imaging community; our team includes core developers in a wide array of tools such as ezomero, napari, ZarrDataset and omero-cli-transfer."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Distributed segmentation for Micro-SAM with Dask\n\n\n\n\n\n\nHPC\n\n\nTutorials\n\n\nMicro-SAM\n\n\n\nTutorial for executing Micro-SAM (or any deep learning) segmentation method on large-scale images using Dask distributed for parallelization\n\n\n\n\n\nFeb 17, 2025\n\n\nFernando Cervantes\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto on HPC\n\n\n\n\n\n\nHPC\n\n\nTutorials\n\n\n\nA tutorial for setting up Quarto in a remote server (HPC)\n\n\n\n\n\nAug 28, 2024\n\n\nFernando Cervantes\n\n\n\n\n\n\n\n\n\n\n\n\nBlog post template\n\n\n\n\n\n\ntemplate\n\n\n\nThis is a description for your blogpost\n\n\n\n\n\nAug 5, 2024\n\n\nErick Ratamero\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "",
    "text": "This guide presents an approach for scaling-up deep learning segmentation methods to be applied at Whole Slide Image (WSI) scales. Whereas this approach is more efficient on High Performance Computing (HPC) environments, the pipeline can be abstracted and executed in different computing environments, even on personal computers. Additionally, the code presented here uses micro-sam as segmentation method; however, this approach can be adapted to execute the inference of any other method.\n\n\nThere are several methods for biological structures segmentation in images, such as Cellpose, StarDist, and U-Net-based methods. This tutorial will focus on Micro-SAM which is derived from the Segment Anything Model (SAM) that uses a Vision Transformer backbone and applies a set of post-processing operations to obtain a segmentation mask. While Micro-SAM already implements a tile-based pipeline that applies the method to sub-regions of microscopy images, this approach has some limitations in terms of memory and computation time needed to compute a whole image since it requires all pixel data to be loaded into memory beforehand.\n\n\n\n\n\n\nImportant\n\n\n\nAt the time this guide was written, the Micro-SAM’s tile-based approach was fully sequential and therefore open for parallelization with the proposed approach.\n\n\n\n\n\nTo scale-up segmentation with Micro-SAM to WSI level, the distributed computation library Dask.distributed is used. The approach consists of encapsulating the segmentation code into a function that can be applied to individual tiles at a time, extracted from the same image. These tiles, also called chunks, are relatively small and its segmentation requires less computational resources than segmenting the whole image at once. The image chunks are distributed and processed with the encapsulated segmentation process by multiple workers in parallel.\nIt is important to point out that each worker has a copy of the Micro-SAM model. The reason is that SAM-based methods register their current input image preventing its use on multiple images at the same time. On the contrary, if a single model is shared among different workers, multiple tiles would be registered without synchronization leading to incorrect and undefined results."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#segmentation-methods",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#segmentation-methods",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "",
    "text": "There are several methods for biological structures segmentation in images, such as Cellpose, StarDist, and U-Net-based methods. This tutorial will focus on Micro-SAM which is derived from the Segment Anything Model (SAM) that uses a Vision Transformer backbone and applies a set of post-processing operations to obtain a segmentation mask. While Micro-SAM already implements a tile-based pipeline that applies the method to sub-regions of microscopy images, this approach has some limitations in terms of memory and computation time needed to compute a whole image since it requires all pixel data to be loaded into memory beforehand.\n\n\n\n\n\n\nImportant\n\n\n\nAt the time this guide was written, the Micro-SAM’s tile-based approach was fully sequential and therefore open for parallelization with the proposed approach."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#distributed-segmentation-approach",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#distributed-segmentation-approach",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "",
    "text": "To scale-up segmentation with Micro-SAM to WSI level, the distributed computation library Dask.distributed is used. The approach consists of encapsulating the segmentation code into a function that can be applied to individual tiles at a time, extracted from the same image. These tiles, also called chunks, are relatively small and its segmentation requires less computational resources than segmenting the whole image at once. The image chunks are distributed and processed with the encapsulated segmentation process by multiple workers in parallel.\nIt is important to point out that each worker has a copy of the Micro-SAM model. The reason is that SAM-based methods register their current input image preventing its use on multiple images at the same time. On the contrary, if a single model is shared among different workers, multiple tiles would be registered without synchronization leading to incorrect and undefined results."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-slurm-job",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-slurm-job",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "2.1. Requesting an interactive job",
    "text": "2.1. Requesting an interactive job\nThe following command allocates a job for a cluster of multiple workers and a single scheduler. This command requests the same number of GPUs as workers are in the cluster; however, that depends on each HPC environment and GPUs availability.\n\n\nbash\n\nsalloc --partition=PARTITION --qos=QOS \\\n    --mem='32gb per worker and 64gb for the scheduler' \\\n    --cpus-per-task='number of workers + 2' \\\n    --gres=gpu:'number of workers' \\\n    --time=6:00:00 srun \\\n    --preserve-env \\\n    --pty /bin/bash\n\n\n\n\n\n\n\nExample command\n\n\n\n\n\nFor a cluster of \\(4\\) workers and \\(4\\) GPU devices (one per worker) the command would be as follows:\n\n\nbash\n\nsalloc --partition=PARTITION --qos=QOS \\\n    --mem=192gb \\\n    --cpus-per-task=6 \\\n    --gres=gpu:4 \\\n    --time=6:00:00 srun \\\n    --preserve-env \\\n    --pty /bin/bash\n\nThe requested CPUs are \\(4+2=6\\) (\\(4\\) workers and \\(2\\) extra for other operations) and memory is \\(32*4 + 64=192\\) GB.\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe PARTITION and QOS (quality of service) names depend on the HPC environment. Make sure that such partition and quality of service enable using GPUs for accelerated computing.\n\n\n\n\n\n\n\n\nTip\n\n\n\nDepending on your HPC environment, allocating the interactive job could involve using different commands, such as sinteractive."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-dask-cluster",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-dask-cluster",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "2.2. Configuring a dask.distributed cluster",
    "text": "2.2. Configuring a dask.distributed cluster\nOnce the interactive job is allocated, set some environment variables to configure the cluster.\n\n\nbash\n\nCLUSTER_HOST=XX.XX.XX.XX\n\n\n\n\n\n\n\nNote\n\n\n\nThe CLUSTER_HOST value can be set to the IP address of the node requested, i.e. $(hostname -i), or simply localhost.\n\n\n\n\nbash\n\nCLUSTER_PORT=8786\n\n\n\n\n\n\n\nNote\n\n\n\nAny free port can be used for creating the cluster, e.g. dask.distributed uses \\(8786\\) by default.\n\n\n\n\nbash\n\nTEMP_DIR=/temporal/directory\n\n\n\n\n\n\n\nNote\n\n\n\nAll temporal files created by the scheduler are stored in TEMP_DIR. This location could be /tmp or any other scratch location."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-scheduler-start",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-scheduler-start",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "2.3. Starting the cluster’s scheduler",
    "text": "2.3. Starting the cluster’s scheduler\n\n\n\n\n\n\nTip\n\n\n\nVerify that the distributed package is installed in the working environment with the following command.\n\nbashsingularity/apptainer\n\n\ndask scheduler --version\n\n\nsingularity exec /path/to/micro-sam-container.sif dask scheduler --version\n\n\n\nIf this does not return the version of the distributed package, follow the dask.distributed’s installation instructions before continuing with this guide.\n\n\nThe scheduler is a process responsible for assigning tiles to available workers in the cluster for their segmentation. Start the cluster’s scheduler as follows.\n\nbashsingularity/apptainer\n\n\ndask scheduler --host $CLUSTER_HOST --port $CLUSTER_PORT &\n\n\nsingularity exec /path/to/micro-sam-container.sif \\\n    dask scheduler --host $CLUSTER_HOST --port $CLUSTER_PORT &\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe scheduler does not require access to GPUs for distributing the pipeline’s tasks even if the workers do have access to them."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-workers-start",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-workers-start",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "2.4. Starting the cluster’s workers",
    "text": "2.4. Starting the cluster’s workers\nA worker is a process responsible for computing the segmentation function on an image chunk by separate. Initiate the workers processes by executing the following command as many times as workers are in the cluster. Note that a specific GPU ID or UUID (Universal Unique ID) will be assigned when starting each worker process.\n\nbashsingularity/apptainer\n\n\nCUDA_VISIBLE_DEVICES='GPU ID or UUID' dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES='GPU ID or UUID' \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAny number of workers can be added to the cluster this way; however, it is good practice to initiate only as many workers as CPUs requested less a pair reserved for the scheduler and other operations. In continuation of Section 2.1 example, this command would be executed four times.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nMemory is distributed by default as the ratio of RAM and CPUs requested with salloc (Section 2.1). Following the example from Section 2.1, \\(192\\) GB are distributed between \\(6\\) CPUs, which is \\(32\\) GB of RAM for each worker and the remainder \\(64\\) GB reserved for the scheduler and other operations.\n\n\nThis guide covers four scenarios to determine what GPU ID/UUID (if any) is assigned when starting each worker. Choose the scenario according to the specifications of the HPC environment used when executing this pipeline.\n\nNo GPU support. There are no GPUs assigned to this job and the process is carried out fully on CPU. For this case remove CUDA_VISIBLE_DEVICES= from the command used to start each worker.\n\n\n\n\n\n\n\nExample for No GPUs\n\n\n\n\n\nThe commands used to start four workers would be the following.\n\nbashsingularity/apptainer\n\n\ndask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\ndask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\ndask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\ndask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\nsingularity exec /path/to/micro-sam-container.sif \\\n    dask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec /path/to/micro-sam-container.sif \\\n    dask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec /path/to/micro-sam-container.sif \\\n    dask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec /path/to/micro-sam-container.sif \\\n    dask worker $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\n\nNote that those commands are exactly the same.\n\n\n\n\nSingle GPU device. This device is shared among all workers and should have enough virtual memory (VRAM) to fit all copies of the model generated by each worker. It is important to point out that because only one device is responsible for computing all operations, its compute latency could be affected negatively.\n\n\n\n\n\n\n\nExample for single GPU\n\n\n\n\n\nSet the environment variable CUDA_VISIBLE_DEVICES to the ID of the only device available for all workers. The device ID can be obtained with the following command.\n\n\nbash\n\necho $CUDA_VISIBLE_DEVICES\n\nFor example, if the only device available has ID \\(0\\),\n$ echo $CUDA_VISIBLE_DEVICES\n0\nthe commands used to start four workers would be the following.\n\nbashsingularity/apptainer\n\n\nCUDA_VISIBLE_DEVICES=0 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=0 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=0 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=0 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=0 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=0 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=0 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=0 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\n\nNote that those commands are exactly the same since these are using the same GPU.\n\n\n\n\nMultiple physical GPU devices. These devices are assigned to different workers, most ideally one GPU device per worker. This would allow us to keep the GPUs computing latency unaffected. Additionally, devices with less virtual memory could be used since only one model will be hosted per GPU.\n\n\n\n\n\n\n\nExample for multiple GPUs\n\n\n\n\n\nSet the environment variable CUDA_VISIBLE_DEVICES to a different ID for each worker. Use the following command to get the GPU IDs.\n\n\nbash\n\necho $CUDA_VISIBLE_DEVICES\n\nFor example, if the device IDs are \\(0\\), \\(1\\), \\(2\\), and \\(3\\),\n$ echo $CUDA_VISIBLE_DEVICES\n0,1,2,3\nthe commands used to start four workers would be the following.\n\nbashsingularity/apptainer\n\n\nCUDA_VISIBLE_DEVICES=0 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=1 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=2 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=3 dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=0 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=1 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=2 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv --env CUDA_VISIBLE_DEVICES=3 \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\n\n\n\n\n\nMulti-Instance GPUs (MIG). This leverages the MIG functionality of certain GPU devices. In this pipeline, a distinct MIG is assigned to each different workers as if these were physical devices.\n\n\n\n\n\n\n\nExample for MIGs\n\n\n\n\n\nSet the environment variable CUDA_VISIBLE_DEVICES to point to a different instance UUID when starting each worker. Instances’ UUIDs can be obtained with the following command.\n\n\nbash\n\nnvidia-smi -L\n\nFor example, if the MIGs’ UUIDs are\n\nMIG-bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb,\nMIG-cccccccc-cccc-cccc-cccc-cccccccccccc,\nMIG-dddddddd-dddd-dddd-dddd-dddddddddddd, and\nMIG-eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee,\n\n$ nvidia-smi -L\nGPU 0: A100-SXM4-40GB (UUID: GPU-aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa)\n  MIG 1g.5gb      Device  0: (UUID: MIG-bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb)\n  MIG 1g.5gb      Device  1: (UUID: MIG-cccccccc-cccc-cccc-cccc-cccccccccccc)\n  MIG 1g.5gb      Device  2: (UUID: MIG-dddddddd-dddd-dddd-dddd-dddddddddddd)\n  MIG 1g.5gb      Device  3: (UUID: MIG-eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee)\nthe commands used to start four workers would be the following.\n\nbashsingularity/apptainer\n\n\nCUDA_VISIBLE_DEVICES=MIG-bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=MIG-cccccccc-cccc-cccc-cccc-cccccccccccc dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=MIG-dddddddd-dddd-dddd-dddd-dddddddddddd dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nCUDA_VISIBLE_DEVICES=MIG-eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\n\nsingularity exec --nv \\\n    --env CUDA_VISIBLE_DEVICES=MIG-bbbbbbbb-bbbb-bbbb-bbbb-bbbbbbbbbbbb \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv \\\n    --env CUDA_VISIBLE_DEVICES=MIG-cccccccc-cccc-cccc-cccc-cccccccccccc \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv \\\n    --env CUDA_VISIBLE_DEVICES=MIG-dddddddd-dddd-dddd-dddd-dddddddddddd \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &\n\nsingularity exec --nv \\\n    --env CUDA_VISIBLE_DEVICES=MIG-eeeeeeee-eeee-eeee-eeee-eeeeeeeeeeee \\\n    /path/to/micro-sam-container.sif dask worker \\\n    $CLUSTER_HOST:$CLUSTER_PORT \\\n    --nthreads 1 \\\n    --local-directory $TEMP_DIR &"
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-segmentation-fun",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-segmentation-fun",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "3.1. Encapsulating micro-sam segmentation function",
    "text": "3.1. Encapsulating micro-sam segmentation function\nThe encapsulated segmentation function comprises two operations: 1) model initialization, and 2) segmentation mask computation. For this method the time required to initialize the deep learning model is negligible compared to the segmentation process. Additionally, by keeping the model within the scope of the segmentation function it is ensured that a single model is instantiated for each worker preventing clashing.\n\n\npython\n\nfrom micro_sam.util import get_device\nfrom micro_sam.automatic_segmentation import get_predictor_and_segmenter, automatic_instance_segmentation\n\ndef sam_segment_chunk(im_chunk, model_type=\"vit_h\", tile_shape=None, halo=None, use_gpu=False, block_info=None):\n    \"\"\"Encapsulated segmentation function\n\n       Parameters\n       ----------\n       im_chunk : array_like\n           Tile or chunk on which the segmentation methods is applied.\n       model_type : str\n           The type of model used for segmentation.\n           Visit https://computational-cell-analytics.github.io/micro-sam/micro_sam.html#finetuned-models for a full list of models.\n       tile_shape : tuple, optional\n           Shape of the tiles for tiled prediction. By default, prediction is run without tiling.\n       halo : tuple, optional\n           Overlap of the tiles for tiled prediction.\n       use_gpu : bool\n           Whether use GPU for acceleration or not.\n       block_info : dict, optional\n           Describes the location of the current chunk in reference to the whole array.\n           This is exclusively used by the `map_blocks` function and does not require to be set by the user.\n\n       Returns\n       -------\n       segmentation_mask : array_like\n           A two-dimensional segmentation mask.\n    \"\"\"\n    sam_predictor, sam_instance_segmenter = get_predictor_and_segmenter(\n        model_type=model_type,\n        device=get_device(\"cuda\" if use_gpu else \"cpu\"),\n        amg=True,\n        checkpoint=None,\n        is_tiled=tile_shape is not None,\n        stability_score_offset=1.0\n    )\n\n    segmentation_mask = automatic_instance_segmentation(\n        predictor=sam_predictor,\n        segmenter=sam_instance_segmenter,\n        input_path=im_chunk[0, :, 0].transpose(1, 2, 0),\n        ndim=2,\n        tile_shape=tile_shape,\n        halo=halo,\n        verbose=False\n    )\n\n    # Offset the segmentation indices to prevent aliasing with other chunks\n    chunk_idx = np.ravel_multi_index(\n        block_info['chunk-location'],\n        block_info['num-chunks']\n    )\n\n    segmentation_mask = np.where(\n        segmentation_mask,\n        segmentation_mask + chunk_idx * (2 ** 32 - 1),\n        0\n    )\n\n    return segmentation_mask\n\n\n\n\n\n\n\nNote\n\n\n\nThe input chunk im_chunk is expected to have axes “TCZYX” following the OME-TIFF specification. This makes this pipeline compatible with images converted to the Zarr format with bioformats2raw converter. When calling the automatic_instance_segmentation function from micro-sam, the input’s axes are squeezed and transposed to have “YXC” order as expected.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe encapsulated function uses Micro-SAM’s tile-based pipeline internally when tile_shape is different from None. That allows us to maintain the behavior close to the original while keeping the process parallelizable."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#opening-an-image-with-dask",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#opening-an-image-with-dask",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "3.2. Opening an image with dask",
    "text": "3.2. Opening an image with dask\nThe input image will be loaded as a dask.array which allows its manipulation in a lazy manner. For more information about the dask.array module visit the official documentation site.\n\n\n\n\n\n\nTip\n\n\n\nLazy loading permits opening only the regions of the image that are being computed at a certain time. Because this process is applied in parallel to individual chunks of the image, instead to the whole image, the overall memory required for segmentation is reduced significatively.\n\n\nAn image can be opened from disk using the tifffile library and be passed to dask.array to retrieve the pixel data lazily. The axes of the image array are ordered following the OME-TIFF specification to “TCZYX”. This order stands for Time, Channel, and the Z, Y, X spatial dimensions.\n\n\n\n\n\n\nTip\n\n\n\nThe tifffile library was historically installed by scikit-image as dependency of its skimage.io module, and is now used as plugin by the imageio library as well.\n\n\n\n\npython\n\nimport tifffile\nimport dask.array as da\n\nim_fp = tifffile.imread(\"/path/to/image/file\", aszarr=True)\nim = da.from_zarr(im_fp)\n\nim = im.transpose(2, 0, 1)[None, :, None, ...]\n\n\n\n\n\n\n\nNote\n\n\n\nBy using aszarr=True argument, the image file is opened as it was a Zarr file allowing to load chunks lazily with the dask.array.from_zarr function.\n\n\n\n3.2.1. Convert input image to Zarr (Optional)\nAlternatively, the input image file can be converted into the Zarr format using converters such as bioformats2raw. That way, the pixel data can be retrieved directly from the file on disk with dask.array.from_zarr. Moreover, if bioformats2raw is used to convert the image, its dimensions will be already in the expected “TCZYX” order.\n\n\npython\n\nimport dask.array as da\n\nim = da.from_zarr(\"/path/to/zarr/file.zarr\", component=\"0/0\")\n\n\n\n3.2.2. Re-chunk tiles to contain all channels\nWhen converting multi-channel images to the Zarr format, it is usual to channels be split into sperate chunks. However, the segmentation function defined in Section 3.1 requires all color channels to be in the same chunk. Therefore, the rechunk method from dask.array.Array objects is used to merge the image’s channels as follows.\n\n\npython\n\nim = im.rechunk({1: -1})\n\n\n\n\n\n\n\nNote\n\n\n\nThe axis at index \\(1\\) corresponds to the Channel dimension in the “TCZYX” ordering.\n\n\n\n\n\n\n\n\nExample change chunk spatial size\n\n\n\n\n\nThe size of the chunks processed by each worker can be modified to match different use-cases, such as smaller or larger chunks depending the available computing resources.\nFor example, if chunks of size \\(4096\\times4096\\) pixels would be used istead of the original’s chunk spatial size, the image would be re-chunked as follows:\n\n\npython\n\nim = im.rechunk({1: -1, 3: 4096, 4: 4096})\n\nThe axes at indices \\(3\\) and \\(4\\) corresponds respectively to the Y and X spatial dimensions in the “TCZYX” ordering."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-create-map-blocks",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-create-map-blocks",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "3.3. Generating a distributed process for lazy computation",
    "text": "3.3. Generating a distributed process for lazy computation\nThe segmentation pipeline is submitted for computation using the map_blocks function from the dask.array module. This function distributes the image tiles across all workers for their segmentation and merges the results back into a single array.\n\n\npython\n\nseg_labels = da.map_blocks(\n    sam_segment_chunk,\n    im,\n    model_type=\"vit_b_lm\",\n    use_gpu=True,\n    tile_shape=[1024, 1024],\n    halo=[256, 256],\n    drop_axis=(0, 1, 2),\n    chunks=im.chunks[-2:],\n    dtype=np.int64,\n    meta=np.empty((0,), dtype=np.int64)\n)\n\n\n\n\n\n\n\nNote\n\n\n\nThe type of model used for segmentation as well as the tile shape and halo parameters can be modified according to the user needs.\n\n\nThe resulting seg_labels array can be set to be stored into a Zarr file directly. This avoids retaining the whole segmentation array on memory unnecessarily.\n\n\npython\n\nseg_labels = seg_labels.to_zarr(\n    \"/path/to/segmentation/output.zarr\",\n    component=\"0/0\",\n    overwrite=True,\n    compute=False\n)\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that the seg_labels.to_zarr method is called with parameter compute=False."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#submitting-graph-for-computation",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#submitting-graph-for-computation",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "3.4. Submitting graph for computation",
    "text": "3.4. Submitting graph for computation\nThe operations in the previous Section 3.3 defined a graph of tasks that are waiting to be executed by a scheduler.\n\n\n\n\n\n\nTip\n\n\n\nThis process is called scheduling and its detailed description can be found at dask’s official documentation.\n\n\nTo use the cluster created in Section 2.3 use the following command.\n\n\npython\n\nfrom dask.distributed import Client\n\nclient = Client('CLUSTER_HOST:CLUSTER_PORT')\n\n\n\n\n\n\n\nImportant\n\n\n\nSet the CLUSTER_HOST and CLUSTER_PORT used in Section 2.2 to create the cluster.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDask will use a single-machine scheduler for executing the graph of tasks by default, so make sure the client is connected before computing the graph.\n\n\nIn interactive notebooks (e.g. jupyter) cluster’s information can be displayed by calling the client as follows.\n\n\npython\n\nclient\n\n\nFinally, execute the pipeline by calling the seg_labels.compute() method.\n\n\npython\n\n_ = seg_labels.compute()\n\n\n\n\n\n\n\nNote\n\n\n\nThe pipeline’s elapsed time depends on the computing resources of the cluster and the size of the input image. It can even take a couple of hours to process a Whole Slide Image (WSI)."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#viewing-cluster-statistics",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#viewing-cluster-statistics",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "3.5. Viewing cluster statistics",
    "text": "3.5. Viewing cluster statistics\nUse the address shown by the client under “Dashboard” to monitor the cluster’s status while the process is running. This tool allows us to visualize the task’s progress and overall usage of the cluster’s resources.\n\n\n\n\n\n\nTip\n\n\n\nThe Dashboard can be accessed through http://CLUSTER_HOST:8787/status. Note that the port \\(8787\\) is used by default; however, in case that port \\(8787\\) is already in use a random port will be generated. The port can be specified when starting the scheduler with the --dashboard-address argument.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThis dashboard may not show the usage of the GPUs, for what the nvidia-smi command can be used instead."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#shutting-down-the-dask.distributed-cluster",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#shutting-down-the-dask.distributed-cluster",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "3.6. Shutting down the dask.distributed cluster",
    "text": "3.6. Shutting down the dask.distributed cluster\nAfter the whole process has finished, execute the command below to shut down the cluster. This will safely terminate the scheduler process and all workers associated with it.\n\n\npython\n\nclient.shutdown()"
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#loading-regions-from-disk-with-dask",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#loading-regions-from-disk-with-dask",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "4.1. Loading regions from disk with dask",
    "text": "4.1. Loading regions from disk with dask\nA similar approach to opening the input image can be used to load the resulting segmentation output in Python or jupyter.\n\n\npython\n\nseg_labels = da.from_zarr(\n    \"/path/to/segmentation/output.zarr\",\n    component=\"0/0\"\n)\n\n\n\n\n\n\n\nTip\n\n\n\nIt is recommended to examine relatively small regions of the input image and resulting segmentation instead of the whole extent of the images to prevent running out of memory.\n\n\n\n\n\n\n\n\nExample of examination of a 512x512 pixels region of interest\n\n\n\n\n\nIn the following code a region of interest (ROI) of size \\(256\\times256\\) pixels at pixel coordinates (\\(512\\), \\(512\\)) is examined.\n\n\npython\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(im[0, :, 0, 512:512 + 256, 512:512 + 256].transpose(1, 2, 0))\nplt.imshow(seg_labels[512:512 + 256, 512:512 + 256], alpha=0.5)\nplt.show()\n\nThe input image is assumed to be stored following the OME specification; therefore, its axes require to be transposed before calling the imshow function."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-comp-pipelines",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-comp-pipelines",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "5.1. Comparing both pipelines",
    "text": "5.1. Comparing both pipelines\nFigure 1 shows the elapsed time taken to segment the sample image using the baseline and the proposed approaches when varying the tile shape parameter. In Figure 2, the count of the objects segmented with the different configurations is shown.\n\nBoxplotBars\n\n\n\n\n\n\n\n\nFigure 1: Elapsed time for segmenting a \\(4096\\times4096\\) pixels sample image with different tile shapes (\\(1024\\times1024\\), \\(512\\times512\\), and \\(256\\times256\\) pixels), using the baseline pipeline and the proposed distributed pipeline.\n\n\n\n\n\n\n\n\nFigure 1: Elapsed time for segmenting a \\(4096\\times4096\\) pixels sample image with different tile shapes (\\(1024\\times1024\\), \\(512\\times512\\), and \\(256\\times256\\) pixels), using the baseline pipeline and the proposed distributed pipeline.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Count of segmented objects in a \\(4096\\times4096\\) pixels sample image obtained with different tile shapes (\\(1024\\times1024\\), \\(512\\times512\\), and \\(256\\times256\\) pixels) using the baseline pipeline and the proposed distributed pipeline.\n\n\n\nAccording to the experimental results, the proposed distributed approach offers an average speed-up of \\(8.10\\) times compared with the baseline approach, and an average increment of \\(5.3 \\%\\) of objects segmented. The increment on segmented objects is due to edge effects that cause objects in adjacent chunks be labeled with different indices. However, there exist tools for handling such edge effects which commonly involve adding overlapping pixels between chunks and relabeling objects in edge regions."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#sec-comp-chunks",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#sec-comp-chunks",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "5.2. Experimenting with input’s chunk sizes",
    "text": "5.2. Experimenting with input’s chunk sizes\nThe size of the chunks handled to each worker also has an effect on the segmentation time. To capture different cases, the sample image was re-chunked to different chunk sizes: \\(2048\\times2048\\), and \\(1024\\times1024\\) pixels per chunk. The time taken to segment the sample image and the count of segmented objects was measured for the different combinations of input’s chunk sizes and segmentation tile shapes. The results are presented in Figure 3 and Figure 4, respectively.\n\nBoxplotBars\n\n\n\n\n\n\n\n\nFigure 3: Elapsed time for segmenting a \\(4096\\times4096\\) pixels sample image using the proposed distributed pipeline for different combinations of input’s chunk size and tile shape.\n\n\n\n\n\n\n\n\nFigure 3: Elapsed time for segmenting a \\(4096\\times4096\\) pixels sample image using the proposed distributed pipeline for different combinations of input’s chunk size and tile shape.\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Count of segmented objects in a \\(4096\\times4096\\) pixels sample image using the proposed distributed pipeline for different combinations of input’s chunk size and tile shape.\n\n\n\nThe experimental results show that overall segmentation time is minimal when the input’s chunk sizes matches the segmentation tile shape. The count of segmented objects is also greater in small chunks compared to larger chunks. However, this is related to single objects labeled with distinct indices by different workers. This problem can be solved in a similar manner as mentioned in Section 5.1 by using overlapping pixels and relabeling algorithms."
  },
  {
    "objectID": "posts/2025-02-17-distributed-micro-sam/index.html#closing-remarks",
    "href": "posts/2025-02-17-distributed-micro-sam/index.html#closing-remarks",
    "title": "Distributed segmentation for Micro-SAM with Dask",
    "section": "5.3. Closing remarks",
    "text": "5.3. Closing remarks\nThis guide introduced a pipeline for scaling-up inference with deep learning methods to a WSI scale applying parallel computing. The experiments have shown a relevant improvement in terms of computation time of the proposed distributed approach with respect to the baseline’s sequential computing. While the computational experiments have been applied only on a sub-image extracted from a WSI, this approach can be similarly applied to the complete extent of a WSI."
  },
  {
    "objectID": "posts/2024-08-05-post-template/index.html",
    "href": "posts/2024-08-05-post-template/index.html",
    "title": "Blog post template",
    "section": "",
    "text": "You can write in sections.\n\n\nMore writing!\n\n\n\nYou can add an image: \nor a link to a webpage.\nYou can also do much, much more (explore the quarto tutorials for how to add plots and data and stuff)."
  },
  {
    "objectID": "posts/2024-08-05-post-template/index.html#section-1",
    "href": "posts/2024-08-05-post-template/index.html#section-1",
    "title": "Blog post template",
    "section": "",
    "text": "You can write in sections.\n\n\nMore writing!\n\n\n\nYou can add an image: \nor a link to a webpage.\nYou can also do much, much more (explore the quarto tutorials for how to add plots and data and stuff)."
  },
  {
    "objectID": "posts/2024-08-28-quarto-on-HPC/index.html",
    "href": "posts/2024-08-28-quarto-on-HPC/index.html",
    "title": "Quarto on HPC",
    "section": "",
    "text": "From the Quarto webpage, it is\n\nAn open-source scientific and technical publishing system\n\nQuarto can be used to create documentation pages, scientific notebooks, professional presentations, and websites (like this one!).\nThis system is useful for creating notes, particularly these containing Python/R code. And in contrast with jupyter notebooks, it offers a wide variety of ways to share these notes."
  },
  {
    "objectID": "posts/2024-08-28-quarto-on-HPC/index.html#option-1.-install-quarto-in-a-conda-virtual-environment",
    "href": "posts/2024-08-28-quarto-on-HPC/index.html#option-1.-install-quarto-in-a-conda-virtual-environment",
    "title": "Quarto on HPC",
    "section": "Option 1. Install Quarto in a conda virtual environment",
    "text": "Option 1. Install Quarto in a conda virtual environment\nFor this option you should have conda installed in your home directory. I recommend using miniforge if you don’t have conda already.\nActivate the virtual environment from where you will use Quarto, i.e. conda activate my-venv (change my-venv with the actual name of your environment).\n\n\n\n\n\n\nNote\n\n\n\nIf you don’t have a virtual environment, start by creating one instead of installing packages in the base environment.\n\n\nInstall quarto from the conda-forge channel as follows.\nconda install -c conda-forge quarto\nYou are ready to render and preview documents from the command line!"
  },
  {
    "objectID": "posts/2024-08-28-quarto-on-HPC/index.html#option-2.-install-quarto-on-your-home-directory",
    "href": "posts/2024-08-28-quarto-on-HPC/index.html#option-2.-install-quarto-on-your-home-directory",
    "title": "Quarto on HPC",
    "section": "Option 2. Install Quarto on your home directory",
    "text": "Option 2. Install Quarto on your home directory\nDownload the Quarto source code for Linux x86 Tarball from their official website into your home directory.\nYou can download the source code by copying the address link of the download, and using wget to download it directly into your home directory.\n\n\n\n\n\n\nTip\n\n\n\nDownload the source code with wget using the following command (change X.X.X to the latest version available).\nwget https://github.com/quarto-dev/quarto-cli/releases/download/vX.X.X/quarto-X.X.X-linux-amd64.tar.gz\n\n\n\n\n\n\n\n\nTip\n\n\n\nIf you are installing it in your PC, you can use the installer that matches your OS.\n\n\nDecompress the source code in a location of your preference within your home directory, like /home/$USER/opt/.\n\n\n\n\n\n\nTip\n\n\n\nYou might need to create a /home/$USER/opt directory if it doesn’t already exist.\nmkdir /home/$USER/opt\n\n\n\n\n\n\n\n\nTip\n\n\n\nDecompress the source code with tar by executing the following command (change the version X.X.X to what you are using first).\ntar -C ~/opt -xvzf quarto-X.X.X-linux-amd64.tar.gz\n\n\nCreate a symbolic link to the Quarto executable inside your /home/$USER/bin directory.\n\n\n\n\n\n\nTip\n\n\n\nYou might need to create a /home/$USER/bin directory if there is not one already as follows.\nmkdir /home/$USER/bin\nIn general, your /home/$USER/bin directory should already be in your PATH environment variable, even if the directory didn’t already exist.\n\n\n\n\n\n\n\n\nTip\n\n\n\nCreate a symbolic link to the quarto executable with the following command (again, change the version X.X.X first).\nln -s /home/$USER/opt/quarto-X.X.X/bin/quarto /home/$USER/bin/quarto\n\n\nCheck that quarto is installed by executing the quarto --version command."
  },
  {
    "objectID": "posts/2024-08-28-quarto-on-HPC/index.html#select-the-jupyter-kernel",
    "href": "posts/2024-08-28-quarto-on-HPC/index.html#select-the-jupyter-kernel",
    "title": "Quarto on HPC",
    "section": "Select the jupyter kernel",
    "text": "Select the jupyter kernel\n\n\n\n\n\n\nNote\n\n\n\nIf you are using Option 1 for installing Quarto in a conda environment, you won’t need to specify the kernel since it will use the one available in that virtual environment. So, you can skip this step.\n\n\nIf you are using Quarto with multiple virtual environments, you can select a specific jupyter kernel for each document. To do that, add jupyter: my-venv to the header of your .qmd file or replace the exiting jupyter: pythonX option if it is already set. Then, add the jupyter kernel of that virtual environment to the list of available kernels with the following commands.\nconda activate my-venv\npython -m ipykernel install --user --name \"my-venv\""
  },
  {
    "objectID": "posts/2024-08-28-quarto-on-HPC/index.html#cache-jupyter-code-cells",
    "href": "posts/2024-08-28-quarto-on-HPC/index.html#cache-jupyter-code-cells",
    "title": "Quarto on HPC",
    "section": "Cache jupyter code cells",
    "text": "Cache jupyter code cells\nEvery time you render a document with Quarto, all code blocks are executed. In the case your code blocks are time-consuming, you might want to use the cache option in quarto documents. The cache option allows to execute cells only when a cell in the document is modified.\n\n\n\n\n\n\nNote\n\n\n\nThat means that modifying the text around cells won’t trigger the execution of the code cells. However, if at least one cell is modified, all cells will be executed.\n\n\nTo use the cache option, install jupyter-cache as follows.\nconda activate my-venv\nconda install -c conda-forge jupyter-cache\n\n\n\n\n\n\nNote\n\n\n\nIf you overrode the QUARTO_PYTHON variable with a virtual environment, install jupyter-cache only in that environment."
  },
  {
    "objectID": "posts/2024-08-28-quarto-on-HPC/index.html#install-the-quarto-extension",
    "href": "posts/2024-08-28-quarto-on-HPC/index.html#install-the-quarto-extension",
    "title": "Quarto on HPC",
    "section": "Install the Quarto extension",
    "text": "Install the Quarto extension\nThe VScode Quarto extension adds markdowns highlighting and commands autocompletion for authoring .qmd files. It can render and preview the quarto markdown documents by hitting Ctrl Shift K (Cmd Shift K on Mac). You can search for the Quarto extension in the Extensions tab (Ctrl Shift X/Cmd Shift X on Mac).\n\n\n\n\n\n\nImportant\n\n\n\nRestart VSCode to make sure that the extension recognizes the Quarto installation in your virtual environments.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you followed Option 1 to install Quarto in a virtual environment the keyboard shortcut might not work. That happens because installing Quarto through conda won’t make it visible to where the extension expects it to be."
  },
  {
    "objectID": "personal_pages/kiya/kiya.html",
    "href": "personal_pages/kiya/kiya.html",
    "title": "Kiya Govek",
    "section": "",
    "text": "Kiya is a Systems Analyst on the Imaging Applications team. At JAX, she develops, deploys, and maintains containerized OMERO instances for imaging data management and public data hosting. She has a background in academic IT software development, computational single-cell next-generation sequencing research in cancer and neurobiology, and dance instruction."
  }
]